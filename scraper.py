# import selenium
from selenium import webdriver
from selenium.webdriver.common.by import By
import pandas as pd
from time import sleep
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import openpyxl
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import urllib.parse
import threading
from queue import Queue

TARGET_WEBSITES = ["https://www.allrecipes.com/recipe/270750/simple-baked-potato/", "https://www.allrecipes.com/scarborough-fair-roasted-vegetables-recipe-11763940", "https://www.allrecipes.com/cheesy-cauliflower-cakes-recipe-11803145", "https://www.allrecipes.com/4-ingredient-egg-avocado-toast-recipe-11763246"] # "https://www.recipes.com"
XPATH_INDEX = {"total-time": '//*[@id="mm-recipes-details_1-0"]/div[1]/div[3]/div[2]', "ingredients": '//*[@id="mm-recipes-structured-ingredients_1-0"]/ul/li', "servings": '//*[@id="mm-recipes-details_1-0"]/div[1]/div[4]/div[2]', "meal-category":'//*[@id="mntl-breadcrumbs__item_2-0"]', "cook-time": '//*[@id="mm-recipes-details_1-0"]/div[1]/div[2]/div[2]', "prep-time": '//*[@id="mm-recipes-details_1-0"]/div[1]/div[1]/div[2]'}
COLUMNS = ["link", "meal-category", "ingredients", "total-time", "servings", "cook-time", "prep-time"]
SITEMAPS = ["https://www.allrecipes.com/sitemap_1.xml", "https://www.allrecipes.com/sitemap_2.xml", "https://www.allrecipes.com/sitemap_3.xml", "https://www.allrecipes.com/sitemap_4.xml"]


ROW_IDENTIFIER = "row"
# TODO: Get all needed data {url, name, category, rating, ingredients, prep_time, cook_time, ready_in_time, calories}

# Create the driver for the selenium browser
# driver = webdriver.Firefox()
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))

# TODO: Make dynamic webpage crawling within target websites base url.
# //*[@id="mntl-taxonomy-nodes__list_1-0"]
# The above is the xml path to the list of different dinner genres
# blank_data = {"link": None, "meal_category": None, "ingredients": None}
df = pd.DataFrame(columns=COLUMNS)
# Category: Things like breakfast, dinner, side dish, and dessert

# Make the get into a reusable function
def get_url(url):
    driver.get(url)
"""# Get an element by Xpath, specifically the list of meal types
# food_types = driver.find_elements(By.XPATH, '//*[@id="mntl-taxonomy-nodes__list_1-0"]')
# print(food_types)
# food_types_list = []

# for food_type in range(len(food_types)):
#    food_types_list.append(food_types[food_type].text)

# print(food_types_list)
"""

def get_elements_by_xpath(xpath):
    """This returns a dictionary containing a list."""
    elements = list(driver.find_elements(By.XPATH, xpath))
    # if (elements == []):
    #     element_dict = {ROW_IDENTIFIER: ["N/A"]}
    #     return element_dict
    # if (function != None):
        # Add a function application 
    element_dict = {ROW_IDENTIFIER: []}
    for element in elements:
        # print(element.text)
        element_dict[ROW_IDENTIFIER].append(element.text)
    return element_dict
# print(ingredients)

# print(ingredients[0].tag_name)

# print(ingredients_list)

df = pd.DataFrame(columns=COLUMNS)

def get_target_urls(sitemap_url):
    driver.get(sitemap_url)

    page_source = driver.page_source

    soup = BeautifulSoup(page_source, "xml")

    # print(soup.prettify())
    urls = soup.find_all("loc")

    # print(urls)
    urls_list = []
    for url in urls:

        clean_url = str(url.contents)[2:-3] # 2:-3 removes brackets and single quotes.
        parsed = urllib.parse.urlparse(str(clean_url))
        # special_url =f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        # print(parsed.path[0:7])
        if (parsed.path[0:8] == "/recipe/"):
            print(clean_url)
            urls_list.append(clean_url)
    return urls_list


# get_url(TARGET_WEBSITES[0])
# element = get_elements_by_xpath(XPATH_INDEX["ingredients"]) # Returns a dictionary containing a list of data.
# print(element)
""" def calculate_details(xpath):
    items = get_elements_by_xpath(xpath)
    details = items[ROW_IDENTIFIER][0].split("\n")
    # parts = s.split(sep)
    # This line was generated by ChatGPT
    result = ["\n".join(details[i:i+2]) for i in range(0, len(details), 2)]
    # first_df = pd.DataFrame()
    temp_dict = {}
    for pair in result:

        # Create a dataframe to be appended to the existing one outside this function
        unit_detail = pair.split("\n")
        temp_dict[unit_detail[0].removesuffix(":")] = unit_detail[1]
        print(f"{unit_detail[0]}:{unit_detail[1]}")
        # temp_df = pd.DataFrame({f"{[unit_detail[0]]}": f"{[unit_detail[1]]}"})
        # temp_df[unit_detail[0]] = unit_detail[1]
        # first_df.add(temp_df)
    return {0:temp_dict}
    # print("\n\n\n", first_df)

        # if (item == "Servings"):
        #     # Find the index of this item in items and get the next one for actual Servings data
        #     curr_index = items.index(item)
        #     print(items[curr_index+1])
"""
# TODO: Make a function to clean up the columns by adding null to any empty values.
def fill_blank_df_fields(df):
    """Run this function on a newly created df to fill empty columns"""
    print(df)
    for column in COLUMNS:
        # print(df[column][ROW_IDENTIFIER])
        if(len(df[column][ROW_IDENTIFIER]) == 0):
            print(f"{column} has an empty entry")
            df.loc[ROW_IDENTIFIER, column] = [None]
    return df
target_websites = []
# Full info on meal xpath: //*[@id="mm-recipes-details_1-0"]/div[1]
for sitemap in SITEMAPS:
    temp_target_websites = get_target_urls(sitemap)
    for value in temp_target_websites:
        target_websites.append(value)
counter = 1
# driver.close()
# if (len(target_websites) > 20):
#     target_websites = target_websites[0:20]
try:
    for url in target_websites:
        # for key in XPATH_INDEX:
        # sleep(2)
        print("running", counter, "out of", len(target_websites))
        counter += 1
        get_url(url)
        # for each of the element dictionaries, I need to check if it is empty and fill it with None if empty
        # Edit the XPATH variables to get a more specific reference.
        # details_dict = calculate_details('//*[@id="mm-recipes-details_1-0"]/div[1]')
        ingredients_dict = get_elements_by_xpath(XPATH_INDEX["ingredients"])
        total_time_dict = get_elements_by_xpath(XPATH_INDEX["total-time"])
        cook_time_dict = get_elements_by_xpath(XPATH_INDEX["cook-time"])
        prep_time_dict = get_elements_by_xpath(XPATH_INDEX["prep-time"])
        serving_dict = get_elements_by_xpath(XPATH_INDEX["servings"])
        meal_category = get_elements_by_xpath(XPATH_INDEX["meal-category"]) 
        temp_df = pd.DataFrame({"link":url, "meal-category":meal_category, "ingredients":ingredients_dict, "total-time": total_time_dict, "servings":serving_dict, "cook-time": cook_time_dict, "prep-time": prep_time_dict}) #TODO: Clean up the ingredients list. There is currently a mismatch in columns filled here and the expected columns.
        temp_df = fill_blank_df_fields(temp_df)
        df = pd.concat([df, temp_df], ignore_index=True)
except Exception as e:
    print("You broke out of the loop early. Writing known data to file. The error is:", e)
# d94427
# print(df)
df.to_csv("./output.csv")



# temp_df = pd.DataFrame([{"link":TARGET_WEBSITES[0], "meal_category":"side-dish", "ingredients": "test string"}])
# df["ingredients"] = zip(ingredients_list)
# df.loc[(len(df))] = [temp_df]
# pd.concat([df, temp_df])
# print(df)
# This input keeps the page alive
# input()
driver.close()
driver.quit()
